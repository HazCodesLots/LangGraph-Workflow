{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f057dbc4-5a92-4751-837f-a08f93f5f7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import Tool\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from typing import Dict, Any, List, Tuple, TypedDict\n",
    "from langgraph.graph import Graph, StateGraph\n",
    "import json\n",
    "\n",
    "\n",
    "mistral = LlamaCpp(\n",
    "    model_path=\"C:/Users/DaysPC/Langgraph/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=2000,\n",
    "    n_ctx=4096,\n",
    "    top_p=0.95,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "\n",
    "def search_web(query: str) -> str:\n",
    "    \"\"\"Simulate web search.\"\"\"\n",
    "    return f\"Found results for: {query}\"\n",
    "\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"Evaluate a mathematical expression.\"\"\"\n",
    "    try:\n",
    "        return str(eval(expression))\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def write_file(content: str) -> str:\n",
    "    \"\"\"Simulate writing content to a file.\"\"\"\n",
    "    return f\"Successfully wrote: {content}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d45ccbc6-7532-48ed-83de-8e58291fdf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlanAgent:\n",
    "    def __init__(self, model: LlamaCpp):\n",
    "        self.model = model\n",
    "        \n",
    "    def create_plan(self, query: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Create a plan by breaking down the query into subtasks.\"\"\"\n",
    "        prompt = f\"\"\"<s>[INST] Break this task into exactly 2 simple steps:\n",
    "        Task: {query}\n",
    "        \n",
    "        Create 2 separate tasks:\n",
    "        1. First find/search for the required information\n",
    "        2. Then perform the calculation or action on that information\n",
    "        \n",
    "        Format as a Python list with exactly 2 dictionaries:\n",
    "        [\n",
    "            {{\"task_id\": 1, \"description\": \"Search for specific information\", \"status\": \"pending\"}},\n",
    "            {{\"task_id\": 2, \"description\": \"Perform calculation with the found information\", \"status\": \"pending\"}}\n",
    "        ]\n",
    "        \n",
    "        Return only the Python list.[/INST]</s>\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.model.invoke(prompt)\n",
    "            \n",
    "            response_clean = response.strip().split('[')[1].split(']')[0]\n",
    "            response_clean = f\"[{response_clean}]\"\n",
    "            tasks = eval(response_clean)\n",
    "            \n",
    "            \n",
    "            if not tasks or len(tasks) == 0:\n",
    "                raise ValueError(\"No tasks created\")\n",
    "                \n",
    "            \n",
    "            cleaned_tasks = []\n",
    "            for i, task in enumerate(tasks, 1):\n",
    "                cleaned_tasks.append({\n",
    "                    \"task_id\": i,\n",
    "                    \"description\": task.get(\"description\", f\"Step {i} of: {query}\"),\n",
    "                    \"status\": \"pending\"\n",
    "                })\n",
    "            return cleaned_tasks\n",
    "            \n",
    "        except Exception as e:\n",
    "            \n",
    "            return [\n",
    "                {\n",
    "                    \"task_id\": 1,\n",
    "                    \"description\": f\"Search for information: {query}\",\n",
    "                    \"status\": \"pending\"\n",
    "                },\n",
    "                {\n",
    "                    \"task_id\": 2,\n",
    "                    \"description\": f\"Process the found information\",\n",
    "                    \"status\": \"pending\"\n",
    "                }\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9fc69cf5-2cc0-463a-adbe-7cd5437f9a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolAgent:\n",
    "    def __init__(self, tools: List[Tool], model: LlamaCpp):\n",
    "        self.model = model\n",
    "        self.calculator = next(t for t in tools if t.name == \"calculator\")\n",
    "        \n",
    "    def get_population_from_model(self, location: str) -> tuple[int, str]:\n",
    "        \"\"\"Use Mistral's knowledge to get population data, parsing JSON response.\"\"\"\n",
    "        import json\n",
    "        \n",
    "        prompt = f\"\"\"[INST] What is the current population of {location}? \n",
    "        Answer ONLY in JSON format, with these keys:\n",
    "        {{\n",
    "          \"population_millions\": float,\n",
    "          \"year\": int\n",
    "        }}\n",
    "        Example: {{\"population_millions\": 8.8, \"year\": 2023}}\n",
    "        Use recent and precise data.[/INST]\"\"\"\n",
    "        \n",
    "        response = self.model.invoke(prompt).strip()\n",
    "        \n",
    "        try:\n",
    "            data = json.loads(response)\n",
    "            population_millions = data[\"population_millions\"]\n",
    "            year = data[\"year\"]\n",
    "            population = int(population_millions * 1_000_000)\n",
    "            return population, f\"Model estimate ({year})\"\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to parse population data from model response: {str(e)}\")\n",
    "        \n",
    "    def execute_task(self, task: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Execute a single task using available tools.\"\"\"\n",
    "        try:\n",
    "            if task[\"task_id\"] == 1:  \n",
    "                \n",
    "                location_prompt = f\"\"\"[INST] Extract just the location name from this text:\n",
    "                {task['description']}\n",
    "                Return only the location name, no other text.[/INST]\"\"\"\n",
    "                \n",
    "                location = self.model.invoke(location_prompt).strip()\n",
    "                \n",
    "                \n",
    "                population, source = self.get_population_from_model(location)\n",
    "                \n",
    "                \n",
    "                if population < 100_000 or population > 350_000_000:  # US population cap\n",
    "                    raise ValueError(\"Population number seems incorrect\")\n",
    "                    \n",
    "                self.current_population = population\n",
    "                \n",
    "                return {\n",
    "                    **task,\n",
    "                    \"status\": \"completed\",\n",
    "                    \"result\": f\"Population of {location}: {population:,}\\nSource: {source}\",\n",
    "                    \"tool_used\": \"model_knowledge\"\n",
    "                }\n",
    "                \n",
    "            elif task[\"task_id\"] == 2:  \n",
    "                if not hasattr(self, 'current_population'):\n",
    "                    raise ValueError(\"Population data not available\")\n",
    "                    \n",
    "                calculation = f\"{self.current_population} * 0.15\"\n",
    "                result = self.calculator.run(calculation)\n",
    "                calculated_value = float(result)\n",
    "                \n",
    "                return {\n",
    "                    **task,\n",
    "                    \"status\": \"completed\",\n",
    "                    \"result\": f\"15% of {self.current_population:,} = {calculated_value:,.2f}\",\n",
    "                    \"tool_used\": \"calculator\"\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                **task,\n",
    "                \"status\": \"failed\",\n",
    "                \"error\": str(e),\n",
    "                \"result\": f\"Error in task execution: {str(e)}\"\n",
    "            }\n",
    "    \n",
    "    def reflect_on_result(self, task: Dict[str, Any]) -> str:\n",
    "        \"\"\"Provide feedback on task execution.\"\"\"\n",
    "        if task.get(\"status\") == \"completed\":\n",
    "            if task[\"task_id\"] == 1:\n",
    "                return \"Successfully retrieved population data\"\n",
    "            elif task[\"task_id\"] == 2:\n",
    "                return \"Successfully calculated percentage\"\n",
    "        return f\"Task {task['task_id']} failed: {task.get('error', 'unknown error')}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "096969f3-89c5-431a-a0a1-45c9bf2b5436",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorkflowState(TypedDict):\n",
    "    query: str\n",
    "    tasks: List[Dict[str, Any]]\n",
    "    current_task_id: int\n",
    "    completed: bool\n",
    "    steps: int\n",
    "    last_node: str\n",
    "\n",
    "def create_workflow(tools: List[Tool], model: LlamaCpp) -> Graph:\n",
    "    \"\"\"Create a simplified workflow that executes tasks sequentially.\"\"\"\n",
    "    \n",
    "    \n",
    "    plan_agent = PlanAgent(model)\n",
    "    tool_agent = ToolAgent(tools, model)\n",
    "    \n",
    "    def plan(state: WorkflowState) -> WorkflowState:\n",
    "        \"\"\"Create initial plan if no tasks exist.\"\"\"\n",
    "        if not state[\"tasks\"]:\n",
    "            state[\"tasks\"] = plan_agent.create_plan(state[\"query\"])\n",
    "            state[\"steps\"] = 0\n",
    "            state[\"last_node\"] = \"plan\"\n",
    "            state[\"current_task_id\"] = 1  \n",
    "        return state\n",
    "    \n",
    "    def execute(state: WorkflowState) -> WorkflowState:\n",
    "        \"\"\"Execute current task.\"\"\"\n",
    "        state[\"steps\"] = state.get(\"steps\", 0) + 1\n",
    "        state[\"last_node\"] = \"execute\"\n",
    "        \n",
    "        \n",
    "        current_task = next((t for t in state[\"tasks\"] \n",
    "                           if t[\"task_id\"] == state[\"current_task_id\"]), None)\n",
    "        \n",
    "        if current_task and current_task[\"status\"] == \"pending\":\n",
    "            \n",
    "            result = tool_agent.execute_task(current_task)\n",
    "            \n",
    "            \n",
    "            state[\"tasks\"] = [\n",
    "                result if t[\"task_id\"] == current_task[\"task_id\"] else t\n",
    "                for t in state[\"tasks\"]\n",
    "            ]\n",
    "            \n",
    "            \n",
    "            if state[\"current_task_id\"] < len(state[\"tasks\"]):\n",
    "                state[\"current_task_id\"] += 1\n",
    "            else:\n",
    "                state[\"completed\"] = True\n",
    "                \n",
    "        return state\n",
    "    \n",
    "    def should_continue(state: WorkflowState) -> Tuple[bool, str]:\n",
    "        \"\"\"Determine next step in workflow.\"\"\"\n",
    "        \n",
    "        if state[\"completed\"]:\n",
    "            return False, \"end\"\n",
    "            \n",
    "        \n",
    "        if state[\"steps\"] >= 10:\n",
    "            state[\"completed\"] = True\n",
    "            return False, \"end\"\n",
    "            \n",
    "        \n",
    "        if not state[\"tasks\"]:\n",
    "            return True, \"plan\"\n",
    "        \n",
    "        \n",
    "        current_task = next((t for t in state[\"tasks\"] \n",
    "                           if t[\"task_id\"] == state[\"current_task_id\"]), None)\n",
    "        \n",
    "        if current_task and current_task[\"status\"] == \"pending\":\n",
    "            return True, \"execute\"\n",
    "        \n",
    "        \n",
    "        if state[\"current_task_id\"] < len(state[\"tasks\"]):\n",
    "            return True, \"execute\"\n",
    "        \n",
    "        \n",
    "        state[\"completed\"] = True\n",
    "        return False, \"end\"\n",
    "    \n",
    "    \n",
    "    workflow = StateGraph(WorkflowState)\n",
    "    \n",
    "    \n",
    "    workflow.add_node(\"plan\", plan)\n",
    "    workflow.add_node(\"execute\", execute)\n",
    "    \n",
    "    \n",
    "    workflow.set_entry_point(\"plan\")\n",
    "    \n",
    "    \n",
    "    workflow.add_conditional_edges(\"plan\", should_continue)\n",
    "    workflow.add_conditional_edges(\"execute\", should_continue)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "def run_workflow(graph: Graph, query: str) -> Dict[str, Any]:\n",
    "    \"\"\"Run workflow with initial query.\"\"\"\n",
    "    initial_state: WorkflowState = {\n",
    "        \"query\": query,\n",
    "        \"tasks\": [],\n",
    "        \"current_task_id\": 1,\n",
    "        \"completed\": False,\n",
    "        \"steps\": 0,\n",
    "        \"last_node\": \"\"\n",
    "    }\n",
    "    return graph.invoke(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "80bfdd97-4d94-4378-87eb-943ea2746658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DaysPC\\anaconda3\\envs\\py310\\lib\\site-packages\\llama_cpp\\llama.py:1240: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Workflow completed!\n",
      "\n",
      "Tasks and Results:\n",
      "\n",
      "Task 1:\n",
      "Description: Search for the population of New Jersey\n",
      "Status: completed\n",
      "Result: Population of New Jersey: 9,200,000\n",
      "Source: Model estimate (2023)\n",
      "\n",
      "Task 2:\n",
      "Description: Calculate 15% of the New Jersey population\n",
      "Status: completed\n",
      "Result: 15% of 9,200,000 = 1,380,000.00\n"
     ]
    }
   ],
   "source": [
    "tools = [\n",
    "    Tool(\n",
    "        name=\"web_search\",\n",
    "        func=search_web,\n",
    "        description=\"Search the web for information.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"calculator\",\n",
    "        func=calculate,\n",
    "        description=\"Calculate mathematical expressions.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"file_writer\",\n",
    "        func=write_file,\n",
    "        description=\"Write content to a file.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "try:\n",
    "    \n",
    "    workflow = create_workflow(tools, mistral)\n",
    "    \n",
    "    \n",
    "    query = \"Find the population of New Jersey and calculate 15% of it\"\n",
    "    \n",
    "    \n",
    "    result = run_workflow(workflow, query)\n",
    "    \n",
    "    print(\"\\nWorkflow completed!\")\n",
    "    print(\"\\nTasks and Results:\")\n",
    "    for task in result[\"tasks\"]:\n",
    "        print(f\"\\nTask {task['task_id']}:\")\n",
    "        print(f\"Description: {task['description']}\")\n",
    "        print(f\"Status: {task['status']}\")\n",
    "        if \"result\" in task:\n",
    "            print(f\"Result: {task['result']}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2a985d-f3d9-492c-b820-105804c6bf7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (py310)",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
